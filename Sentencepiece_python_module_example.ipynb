{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shichaog/GPT/blob/main/Sentencepiece_python_module_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUS0QWGVA5rJ"
      },
      "source": [
        "# Sentencepiece python module\n",
        "This notebook decribes comprehensive examples of sentencepiece Python module. since Python module calls C++ API through SWIG, this document is also useful for developing c++ client.\n",
        "\n",
        "This is a copy implementation of Sentencepiece example from google github. But with Chinese examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lYqj4vPDQOe"
      },
      "source": [
        "# mount Google Drive for reading《遮天》.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U0jdVKGCzNG",
        "outputId": "6baf78fc-81b2-4349-f8ba-ecbc64cfc3cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Colab Notebooks\n",
            "zhetian.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My Drive/Colab Notebooks/\n",
        "!ls zhe*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-aovPC9DYiv"
      },
      "source": [
        "# Install sentencepiece\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GLtBnxUDXdv",
        "outputId": "9bae56e3-f568-4ef4-8783-1f775d2494a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_msbHMpXDri4"
      },
      "source": [
        "# Basic end-to-end exmaple\n",
        "Chinese don't use space to seprate words. The next code segments is using **unigram** method.When --model_type=unigram (default) is used, we can perform sampling and n-best segmentation for data augmentation. See subword regularization paper [kudo18] for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "slj_zzsLDvMt",
        "outputId": "fdbd0d1e-0df4-43fa-ac97-cc94e6904f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['▁', '叶', '凡', '经', '历', '九', '龙', '抬', '棺']\n",
            "[6, 388, 359, 295, 606, 117, 101, 967, 383]\n",
            "叶凡经历九龙抬棺\n",
            "叶凡经历九\n"
          ]
        }
      ],
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "# train sentencepiece model from `zhetian.txt` and makes `m.model` and `m.vocab`\n",
        "# `m.vocab` is just a reference. not used in the segmentation.\n",
        "spm.SentencePieceTrainer.train('--input=zhetian.txt --model_prefix=m --vocab_size=3439')\n",
        "\n",
        "# makes segmenter instance and loads the model file (m.model)\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.load('m.model')\n",
        "\n",
        "# encode: text => id\n",
        "print(sp.encode_as_pieces('叶凡经历九龙抬棺'))\n",
        "print(sp.encode_as_ids('叶凡经历九龙抬棺'))\n",
        "\n",
        "# decode: id => text\n",
        "print(sp.decode_pieces(['▁', '叶', '凡', '经', '历', '九', '龙', '抬', '棺']))\n",
        "print(sp.decode_ids([388, 359, 295, 606, 117]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gjVkWyxvEYSA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66261607-3ee4-432b-b14d-3b1640fefcf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3439\n",
            "0\n",
            "<unk> False\n",
            "<s> True\n",
            "</s> True\n"
          ]
        }
      ],
      "source": [
        "# returns vocab size\n",
        "print(sp.get_piece_size())\n",
        "\n",
        "\n",
        "# returns 0 for unknown tokens (we can change the id for UNK)\n",
        "print(sp.piece_to_id('__MUST_BE_UNKNOWN__'))\n",
        "\n",
        "# <unk>, <s>, </s> are defined by default. Their ids are (0, 1, 2)\n",
        "# <s> and </s> are defined as 'control' symbol.\n",
        "for id in range(3):\n",
        "  print(sp.id_to_piece(id), sp.is_control(id))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BPE (Byte pair encoding) model\n",
        "Sentencepiece supports BPE (byte-pair-encoding) for subword segmentation with --model_type=bpe flag. We do not find empirical differences in translation quality between BPE and unigram model, but unigram model can perform sampling and n-best segmentation. See subword regularization paper [kudo18] for more detail."
      ],
      "metadata": {
        "id": "Easeuo69Pe8r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train('--input=zhetian.txt --model_prefix=m_bpe --vocab_size=3439 --model_type=bpe')\n",
        "sp_bpe = spm.SentencePieceProcessor()\n",
        "sp_bpe.load('m_bpe.model')\n",
        "\n",
        "print('*** BPE ***')\n",
        "print(sp_bpe.encode_as_pieces('叶凡经历九龙抬棺'))\n",
        "print(sp.encode_as_ids('叶凡经历九龙抬棺'))\n",
        "print(sp_bpe.nbest_encode_as_pieces('叶凡经历九龙抬棺', 5))  # returns an empty list."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hb-K46MPZDj",
        "outputId": "0012da1b-6705-4704-9136-dfee4d4860ea"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** BPE ***\n",
            "['▁', '叶', '凡', '经', '历', '九', '龙', '抬', '棺']\n",
            "[6, 388, 359, 295, 606, 117, 101, 967, 383]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Character and word model\n",
        "Sentencepiece supports character and word segmentation with --model_type=char and --model_type=character flags.\n",
        "\n",
        "In word segmentation, sentencepiece just segments tokens with whitespaces, so the input text must be pre-tokenized. We can apply different segmentation algorithm transparently without changing pre/post processors."
      ],
      "metadata": {
        "id": "Oe0iuFnzP8m0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train('--input=zhetian.txt --model_prefix=m_char --model_type=char --vocab_size=3439')\n",
        "\n",
        "sp_char = spm.SentencePieceProcessor()\n",
        "sp_char.load('m_char.model')\n",
        "\n",
        "print(sp_char.encode_as_pieces('叶凡经历九龙抬棺'))\n",
        "print(sp_char.encode_as_ids('叶凡经历九龙抬棺'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7-s5vz-QANS",
        "outputId": "df662e03-5f16-4a64-e50f-a5858a3c5912"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', '叶', '凡', '经', '历', '九', '龙', '抬', '棺']\n",
            "[5, 22, 23, 151, 606, 189, 146, 1134, 520]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spm.SentencePieceTrainer.train('--input=zhetian.txt --model_prefix=m_word --model_type=char --vocab_size=3439')\n",
        "\n",
        "sp_char = spm.SentencePieceProcessor()\n",
        "sp_char.load('m_word.model')\n",
        "\n",
        "print(sp_char.encode_as_pieces('叶凡经历九龙抬棺'))\n",
        "print(sp_char.encode_as_ids('叶凡经历九龙抬棺'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KbrQGUDQLiD",
        "outputId": "e051a2f3-07dd-4d78-8932-c47ce901bac0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁', '叶', '凡', '经', '历', '九', '龙', '抬', '棺']\n",
            "[5, 22, 23, 151, 606, 189, 146, 1134, 520]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeTAKPJmftaVYAewN2Cik9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}